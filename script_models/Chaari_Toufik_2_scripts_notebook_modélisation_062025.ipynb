{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK4E0Q_wFqdt"
      },
      "source": [
        "## Livrable 1\n",
        "\n",
        "Ensemble des scripts pour réaliser les trois approches (classique, modèle sur mesure avancé, modèle avancé BERT).\n",
        "Ce livrable intégrera la gestion des expérimentations avec l’outil MLFlow (tracking des expérimentations, enregistrement des modèles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcD0wbnFNjFS"
      },
      "source": [
        "## Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ocJaeEbNhpW",
        "outputId": "b1754476-8b7c-47cd-e187-a79823026efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/sentiment140\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "\n",
        "# Download latest version of the Sentiment140 dataset\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# The dataset typically includes a CSV file; specify its path\n",
        "# Adjust the file name if it's different in your downloaded dataset\n",
        "csv_file = f\"{path}/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "# The Sentiment140 dataset has no header, so we specify the column names\n",
        "columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
        "df_raw = pd.read_csv(csv_file, encoding='ISO-8859-1', names=columns)\n",
        "df = df_raw.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZnn2yhwMzoY"
      },
      "source": [
        "# Approche classique 1 : Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Skht7iFM5Sq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvhpqf0SNJUX",
        "outputId": "98c03c1f-9f14-4104-9f4e-ed1293c4268e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick brown fox run', 'test sentenc']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Chargement du modèle spaCy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "\n",
        "\n",
        "def lemmatize_and_clean(texts, use_stemming=True, use_lemmatization=True, batch_size=1000):\n",
        "    \"\"\"\n",
        "    Praitement du texte via spaCy en recourant à la lemmatization et/ou NLTK pour le stemming,\n",
        "    suppression de la punctuation and des stop words.\n",
        "\n",
        "    Parametres:\n",
        "    - texts: Liste de textes à traiter\n",
        "    - use_stemming: si True, appliquer stemming (default: True)\n",
        "    - use_lemmatization: Si True, appliquer lemmatization (default: True)\n",
        "\n",
        "    Returns:\n",
        "    - List comprenant les textes traités/ néttoyée\n",
        "    \"\"\"\n",
        "    # Initialisation de PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    cleaned_texts = []\n",
        "    for doc in nlp.pipe(texts, batch_size=batch_size, disable=['parser', 'ner']):\n",
        "        tokens = []\n",
        "        for token in doc:\n",
        "            if token.is_punct or token.is_stop:\n",
        "                continue\n",
        "            word = token.text\n",
        "            if use_lemmatization:\n",
        "                word = token.lemma_\n",
        "            if use_stemming:\n",
        "                word = stemmer.stem(word)\n",
        "            tokens.append(word)\n",
        "        cleaned_text = ' '.join(tokens)\n",
        "        cleaned_texts.append(cleaned_text)\n",
        "\n",
        "    return cleaned_texts\n",
        "\n",
        "# Example cas d'usage\n",
        "texts = [\"The quick brown foxes are running!\", \"This is another test sentence.\"]\n",
        "result = lemmatize_and_clean(texts)\n",
        "print(result)\n",
        "# Output: ['quick brown fox run', 'test sentence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDCg92hpNPWr"
      },
      "outputs": [],
      "source": [
        "#cleaned_text=lemmatize_and_clean(df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "h0e1GlDwOSvq",
        "outputId": "b89689c0-50cb-4149-d8d9-c0f9f9180075"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport pandas as pd\\nfrom google.colab import drive\\n\\n# Mount Google Drive\\ndrive.mount(\\'/content/drive\\')\\n\\n# Create a DataFrame with cleaned text and target\\noutput_df = pd.DataFrame({\\n    \\'cleaned_text\\': cleaned_text,\\n    \\'target\\': df[\\'target\\']\\n})\\n\\n# Save to CSV\\noutput_path = \\'/content/drive/MyDrive/Colab_Notebooks/PROJET7/cleandata.csv\\'\\noutput_df.to_csv(output_path, index=False)\\n\\nprint(f\"Data saved to {output_path}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "'''\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a DataFrame with cleaned text and target\n",
        "output_df = pd.DataFrame({\n",
        "    'cleaned_text': cleaned_text,\n",
        "    'target': df['target']\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "output_path = '/content/drive/MyDrive/Colab_Notebooks/PROJET7/cleandata.csv'\n",
        "output_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Data saved to {output_path}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP8ctTPoOV4L"
      },
      "source": [
        "## Le texte a subi un prétraitement et a été sauveguardé dans un fichier distinct (traitement long a éxecuter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ofDUgu1BOXW7",
        "outputId": "e0bf98b9-8a0c-4f4f-f100-90c1af47eb5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                        cleaned_text  target\n",
              "0  @switchfoot http://twitpic.com/2y1zl awww bumm...       0\n",
              "1  upset updat facebook text cri result   school ...       0\n",
              "2  @kenichan dive time ball manag save 50   rest ...       0\n",
              "3                          bodi feel itchi like fire       0\n",
              "4                         @nationwideclass behav mad       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-756515ac-8630-42dc-b26d-d40d337a8efd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl awww bumm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>upset updat facebook text cri result   school ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@kenichan dive time ball manag save 50   rest ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bodi feel itchi like fire</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@nationwideclass behav mad</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-756515ac-8630-42dc-b26d-d40d337a8efd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-756515ac-8630-42dc-b26d-d40d337a8efd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-756515ac-8630-42dc-b26d-d40d337a8efd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-14b759ac-99e9-4834-b801-217e5eef1add\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-14b759ac-99e9-4834-b801-217e5eef1add')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-14b759ac-99e9-4834-b801-217e5eef1add button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_cleaned\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"upset updat facebook text cri result   school today blah\",\n          \"@nationwideclass behav mad\",\n          \"@kenichan dive time ball manag save 50   rest bound\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_cleaned = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/PROJET7/cleandata.csv')\n",
        "\n",
        "display(df_cleaned.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY3HrM7POcyF"
      },
      "outputs": [],
      "source": [
        "# Étape 1 : Division des données\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_cleaned['cleaned_text'], df_cleaned['target'], test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_xMJ2_yOe_m",
        "outputId": "d6a5784b-7b64-4093-af4c-4cfb10774c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of NaN values in 'text' column: 0.0000\n"
          ]
        }
      ],
      "source": [
        "proportion_nan_target = df_cleaned['target'].isnull().sum() / len(df_cleaned['target'])\n",
        "print(f\"Proportion of NaN values in 'text' column: {proportion_nan_target:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7xVDk0LOh-s",
        "outputId": "6ed93ad0-cc09-40e9-9348-aa2bd21b4ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of NaN values in 'text' column: 0.0007\n"
          ]
        }
      ],
      "source": [
        "proportion_nan_text = df_cleaned['cleaned_text'].isnull().sum() / len(df_cleaned['cleaned_text'])\n",
        "print(f\"Proportion of NaN values in 'text' column: {proportion_nan_text:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kHneYamOoTs",
        "outputId": "1223f633-8345-4a68-c2ae-deb3a010e288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN in 'text': 1152\n",
            "Number of empty strings in 'text': 0\n",
            "Number of NaN after cleaning: 0\n",
            "X_train_vec shape: (1279078, 550064)\n",
            "X_test_vec shape: (319770, 550064)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Check for NaN or empty values in 'text' column\n",
        "print(\"Number of NaN in 'text':\", df_cleaned['cleaned_text'].isna().sum())\n",
        "print(\"Number of empty strings in 'text':\", (df_cleaned['cleaned_text'] == '').sum())\n",
        "\n",
        "# Step 2: Handle NaN values by replacing with empty strings\n",
        "df_cleaned = df_cleaned.dropna(subset=['cleaned_text'])# (Alternatively, you could drop NaN rows with)\n",
        "#df_cleaned['text'] = df_cleaned['text'].fillna('')  # Replace NaN with empty string\n",
        "\n",
        "# Step 3: Verify no NaN values remain\n",
        "print(\"Number of NaN after cleaning:\", df_cleaned['cleaned_text'].isna().sum())\n",
        "\n",
        "# Step 4: Perform train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_cleaned['cleaned_text'],\n",
        "    df_cleaned['target'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit and transform X_train\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# Transform X_test (no fit to avoid data leakage)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"X_train_vec shape:\", X_train_vec.shape)\n",
        "print(\"X_test_vec shape:\", X_test_vec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmOlO4k4OrTT"
      },
      "outputs": [],
      "source": [
        "# Étape 3 : Encodage des catégories\n",
        "# Initialiser LabelEncoder pour convertir les catégories textuelles en nombres\n",
        "encoder = LabelEncoder()\n",
        "# Ajuster (fit) et transformer y_train en nombres\n",
        "y_train_enc = encoder.fit_transform(y_train)\n",
        "# Transformer y_test avec le même encodage (pas fit, pour cohérence)\n",
        "y_test_enc = encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder.classes_)      # should output: array([0, 4])\n",
        "print(set(y_train_enc))      # should be {0, 1}\n",
        "print(set(y_test_enc))       # should also be {0, 1}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VjRECzyeJ_J",
        "outputId": "c29b32ee-5493-444d-e523-b2a276372f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 4]\n",
            "{np.int64(0), np.int64(1)}\n",
            "{np.int64(0), np.int64(1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mlflow"
      ],
      "metadata": {
        "id": "KWOaBV1LhG4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBDzc9m0wLcy"
      },
      "source": [
        "## XGBOOST BAG OF WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ImxOAgYGTMee",
        "outputId": "14c545a8-ada5-41fb-84a7-5e5073cd5a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive is already mounted.\n",
            "5000/tcp:             8316  8317\n",
            "🖥️  Interface MLflow : https://5000-m-hm-1nk77asvti4v9-a.asia-east1-0.prod.colab.dev\n",
            "✅  BoW_XGB_final — acc = 0.7534\n",
            "🏃 View run final_BoW_XGB at: http://127.0.0.1:5000/#/experiments/547837938429167033/runs/e9ad18e1367e476e96e62dc28efc449a\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/547837938429167033\n",
            "\n",
            "🏁  Entraînement terminé — consulte le run « final_BoW_XGB » dans l’UI MLflow : https://5000-m-hm-1nk77asvti4v9-a.asia-east1-0.prod.colab.dev\n"
          ]
        }
      ],
      "source": [
        "# ────────────────────────────────────────────────\n",
        "# ①  MONTER GOOGLE DRIVE\n",
        "# ────────────────────────────────────────────────\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# ②  DÉMARRER MLFLOW (backend persistant sur Drive)\n",
        "# ────────────────────────────────────────────────\n",
        "import shlex, os, time, mlflow, pathlib\n",
        "\n",
        "PORT = 5000\n",
        "!fuser -k {PORT}/tcp || true          # libère le port s'il était occupé\n",
        "\n",
        "BACKEND = \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/MLflowStore\"  # ✅ sans espace\n",
        "os.makedirs(BACKEND, exist_ok=True)   # crée le dossier si besoin\n",
        "\n",
        "quoted = shlex.quote(BACKEND)         # ajoute quotes (sécurité)\n",
        "\n",
        "# redirection vers un log pour débogage éventuel\n",
        "get_ipython().system_raw(\n",
        "    f\"mlflow server \"\n",
        "    f\"--backend-store-uri {quoted} \"\n",
        "    f\"--default-artifact-root {quoted} \"\n",
        "    f\"--host 0.0.0.0 --port {PORT} --workers 1 \"\n",
        "    f\"> mlflow.log 2>&1 &\"\n",
        ")\n",
        "\n",
        "time.sleep(4)                         # on laisse le serveur se lancer\n",
        "\n",
        "# URL proxy Colab\n",
        "from google.colab import output, widgets\n",
        "ui_url = output.eval_js(f\"google.colab.kernel.proxyPort({PORT})\")\n",
        "print(\"🖥️  Interface MLflow :\", ui_url)\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# ③  CONFIGURATION CLIENT & EXPÉRIENCE\n",
        "# ────────────────────────────────────────────────\n",
        "mlflow.set_tracking_uri(f\"http://127.0.0.1:{PORT}\")\n",
        "mlflow.set_experiment(\"BoW_XGB_Binary\")\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# ④  PRÉPARATION DES DONNÉES (TF-IDF ou CountVectorizer)\n",
        "#     — ici on suppose X_train, X_test, y_train_enc, y_test_enc,\n",
        "#       vectorizer, encoder déjà présents dans l'environnement.\n",
        "#     Sinon, adapte la partie ci-dessous.\n",
        "# ────────────────────────────────────────────────\n",
        "# Ex. :\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec  = vectorizer.transform(X_test)\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# ⑤  ENTRAÎNEMENT UNIQUE + LOG MINIMAL\n",
        "# ────────────────────────────────────────────────\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle, pathlib, mlflow\n",
        "\n",
        "# hyper-paramètres définitifs\n",
        "best_params = {\"n_estimators\": 400, \"learning_rate\": 0.20, \"max_depth\": 6}\n",
        "\n",
        "class_names = [str(c) for c in encoder.classes_]   # ex. ['0', '4']\n",
        "\n",
        "# dossier pickles\n",
        "pkl_dir = pathlib.Path(\"/content/drive/MyDrive/Colab_Notebooks/PROJET7/pickles\")\n",
        "pkl_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with mlflow.start_run(run_name=\"final_BoW_XGB\"):\n",
        "\n",
        "    # — modèle\n",
        "    model = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        **best_params\n",
        "    ).fit(X_train_vec, y_train_enc)\n",
        "\n",
        "\n",
        "    # — prédictions & métriques\n",
        "    y_proba = model.predict_proba(X_test_vec)[:, 1]   # probability of class “1” (i.e. original “4”)\n",
        "    y_pred  = (y_proba > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test_enc, y_pred)\n",
        "    rpt = classification_report(\n",
        "        y_test_enc, y_pred,\n",
        "        target_names=class_names,\n",
        "        output_dict=True, zero_division=0\n",
        "    )\n",
        "\n",
        "    # — log dans MLflow\n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"accuracy\", acc)\n",
        "    for lbl in class_names:\n",
        "        mlflow.log_metric(f\"precision_{lbl}\", rpt[lbl][\"precision\"])\n",
        "        mlflow.log_metric(f\"recall_{lbl}\",    rpt[lbl][\"recall\"])\n",
        "        mlflow.log_metric(f\"f1_{lbl}\",        rpt[lbl][\"f1-score\"])\n",
        "\n",
        "    # — sauvegarde modèle\n",
        "    pkl_path = pkl_dir / \"BoW_XGB_final.pkl\"\n",
        "    with open(pkl_path, \"wb\") as f:\n",
        "        pickle.dump(\n",
        "            {\"model\": model, \"vectorizer\": vectorizer, \"encoder\": encoder},\n",
        "            f\n",
        "        )\n",
        "    mlflow.log_artifact(str(pkl_path))\n",
        "\n",
        "    print(f\"✅  BoW_XGB_final — acc = {acc:.4f}\")\n",
        "\n",
        "print(\"\\n🏁  Entraînement terminé — consulte le run « final_BoW_XGB » dans l’UI MLflow :\", ui_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbLlwh14v43n"
      },
      "outputs": [],
      "source": [
        "import shlex, os, time, mlflow, pathlib\n",
        "\n",
        "PORT = 5000\n",
        "!fuser -k {PORT}/tcp || true          # libère le port s'il était occupé\n",
        "\n",
        "BACKEND = \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/MLflowStore\"  # ✅ sans espace\n",
        "os.makedirs(BACKEND, exist_ok=True)   # crée le dossier si besoin\n",
        "\n",
        "quoted = shlex.quote(BACKEND)         # ajoute quotes (sécurité)\n",
        "\n",
        "# redirection vers un log pour débogage éventuel\n",
        "get_ipython().system_raw(\n",
        "    f\"mlflow server \"\n",
        "    f\"--backend-store-uri {quoted} \"\n",
        "    f\"--default-artifact-root {quoted} \"\n",
        "    f\"--host 0.0.0.0 --port {PORT} --workers 1 \"\n",
        "    f\"> mlflow.log 2>&1 &\"\n",
        ")\n",
        "\n",
        "time.sleep(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi9Fz-L5lZdF"
      },
      "source": [
        "## Approche TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53lR5kL7mwUH",
        "outputId": "bc3f6cd5-d6cd-43a4-fd17-6c2b56b9781c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/20 16:05:37 INFO mlflow.tracking.fluent: Experiment with name 'TFIDF_XGBoost_simplified' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  TF-IDF fixed — acc=0.6628 | f1_0=0.566 | f1_4=0.724\n",
            "🏃 View run TFIDF_fixed at: http://127.0.0.1:5000/#/experiments/642038648378918568/runs/d7bf45aac1d0440b86f496161dbc1dd2\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/642038648378918568\n"
          ]
        }
      ],
      "source": [
        "# ────────────────────────────────────────────────\n",
        "# 0. Suppositions\n",
        "# ────────────────────────────────────────────────\n",
        "# • serveur MLflow tourne déjà (http://127.0.0.1:5000)\n",
        "# • X_train, X_test, y_train_enc, y_test_enc, encoder existent\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 1. Vectorisation TF-IDF (1–2-gram)\n",
        "# ────────────────────────────────────────────────\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))        # + stop_words='english' si besoin\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec  = vectorizer.transform(X_test)\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 2. Config client MLflow\n",
        "# ────────────────────────────────────────────────\n",
        "import mlflow, pathlib, pickle\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "mlflow.set_experiment(\"TFIDF_XGBoost_simplified\")\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 3. Entraînement + log\n",
        "# ────────────────────────────────────────────────\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "params = dict(\n",
        "    n_estimators = 100,\n",
        "    learning_rate= 0.05,\n",
        "    max_depth    = 3,\n",
        "    reg_lambda   = 2.0,\n",
        "    reg_alpha    = 1.0\n",
        ")\n",
        "\n",
        "class_names = [str(c) for c in encoder.classes_]        # ['0', '4'] par ex.\n",
        "\n",
        "with mlflow.start_run(run_name=\"TFIDF_fixed\"):\n",
        "\n",
        "    # — modèle\n",
        "    model = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        **params\n",
        "    ).fit(X_train_vec, y_train_enc)\n",
        "\n",
        "\n",
        "    # — prédictions & métriques\n",
        "    y_proba = model.predict_proba(X_test_vec)[:, 1]   # probability of class “1” (i.e. original “4”)\n",
        "    y_pred  = (y_proba > 0.5).astype(int)\n",
        "\n",
        "    acc  = accuracy_score(y_test_enc, y_pred)\n",
        "    rpt  = classification_report(\n",
        "              y_test_enc, y_pred,\n",
        "              target_names=class_names,\n",
        "              output_dict=True,\n",
        "              zero_division=0\n",
        "           )\n",
        "\n",
        "    # — log dans MLflow\n",
        "    mlflow.log_params(params)\n",
        "    mlflow.log_metric(\"accuracy\", acc)\n",
        "    for lbl in class_names:\n",
        "        mlflow.log_metric(f\"precision_{lbl}\", rpt[lbl][\"precision\"])\n",
        "        mlflow.log_metric(f\"recall_{lbl}\",    rpt[lbl][\"recall\"])\n",
        "        mlflow.log_metric(f\"f1_{lbl}\",        rpt[lbl][\"f1-score\"])\n",
        "\n",
        "    # — sauvegarde pickle (Drive + artefact)\n",
        "    pkl_path = pathlib.Path(\n",
        "        \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/TFIDF_fixed.pkl\"\n",
        "    )\n",
        "    with open(pkl_path, \"wb\") as f:\n",
        "        pickle.dump(\n",
        "            {\"model\": model, \"vectorizer\": vectorizer, \"encoder\": encoder}, f\n",
        "        )\n",
        "    mlflow.log_artifact(str(pkl_path))\n",
        "\n",
        "    print(f\"✅  TF-IDF fixed — acc={acc:.4f} | f1_0={rpt['0']['f1-score']:.3f} | f1_4={rpt['4']['f1-score']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8E4VprI3RUn"
      },
      "source": [
        "## Approche word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "Y6rdmrdT3vDj",
        "outputId": "5dcca97a-7e84-40da-d0f6-0a781ed2e241",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m435.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b695a2b59ab04b999cde26b6b4f5093a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.1\n",
            "    Uninstalling numpy-2.3.1:\n",
            "      Successfully uninstalled numpy-2.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "fdbb6edb10a649b2b8e70ded37e3ae6f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!pip uninstall gensim numpy -y\n",
        "#!pip install numpy\n",
        "#!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shlex, os, time, mlflow, pathlib\n",
        "\n",
        "PORT = 5000\n",
        "!fuser -k {PORT}/tcp || true          # libère le port s'il était occupé\n",
        "\n",
        "BACKEND = \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/MLflowStore\"  # ✅ sans espace\n",
        "os.makedirs(BACKEND, exist_ok=True)   # crée le dossier si besoin\n",
        "\n",
        "quoted = shlex.quote(BACKEND)         # ajoute quotes (sécurité)\n",
        "\n",
        "# redirection vers un log pour débogage éventuel\n",
        "get_ipython().system_raw(\n",
        "    f\"mlflow server \"\n",
        "    f\"--backend-store-uri {quoted} \"\n",
        "    f\"--default-artifact-root {quoted} \"\n",
        "    f\"--host 0.0.0.0 --port {PORT} --workers 1 \"\n",
        "    f\"> mlflow.log 2>&1 &\"\n",
        ")\n",
        "\n",
        "time.sleep(4)"
      ],
      "metadata": {
        "id": "y8NtQWOLplwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdS6T3-m3UU2",
        "outputId": "362d73c9-5103-49cf-d19e-da9b80b2f7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/20 16:30:01 INFO mlflow.tracking.fluent: Experiment with name 'W2V_XGBoost' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  W2V_fixed — acc=0.7482 | f1_0=0.752 | f1_4=0.745\n",
            "🏃 View run W2V_fixed at: http://127.0.0.1:5000/#/experiments/140404627774248766/runs/9891fe88bbf0470abd8e5f220264fd58\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/140404627774248766\n",
            "\n",
            "🏁  Run terminé — retrouvez-le dans l’UI MLflow (onglet *W2V_XGBoost*).\n"
          ]
        }
      ],
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# 0. INSTALL génim / kagglehub (si besoin) – une seule fois\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "!pip install -q gensim kagglehub\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 1. DATASET Sentiment140\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "import kagglehub, pandas as pd, gensim, numpy as np, mlflow, pathlib, pickle, os, shlex, time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Téléchargement\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "csv_file = f\"{path}/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "cols = ['target','id','date','flag','user','text']\n",
        "df = pd.read_csv(csv_file, encoding='ISO-8859-1', names=cols)\n",
        "\n",
        "# Pré-tokenisation simple (minuscule, remove punct, etc.)\n",
        "tokens = df['text'].apply(gensim.utils.simple_preprocess)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tokens, df['target'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Encode labels (0→0, 4→1)\n",
        "encoder   = LabelEncoder()\n",
        "y_train_e = encoder.fit_transform(y_train)\n",
        "y_test_e  = encoder.transform(y_test)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 2. Word2Vec 300-d\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "w2v_model = gensim.models.Word2Vec(\n",
        "    sentences=X_train,\n",
        "    vector_size=300,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "def doc_vector(tokens, model):\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_vec = np.vstack([doc_vector(doc, w2v_model) for doc in X_train])\n",
        "X_test_vec  = np.vstack([doc_vector(doc, w2v_model) for doc in X_test])\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 3. MLflow : expérience Word2Vec\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "mlflow.set_experiment(\"W2V_XGBoost\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"W2V_fixed\"):\n",
        "\n",
        "    params = dict(\n",
        "        n_estimators = 161,\n",
        "        learning_rate= 0.05,\n",
        "        max_depth    = 4,\n",
        "        subsample    = 0.6,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha    = 0.0,\n",
        "        reg_lambda   = 2.0,\n",
        "        n_jobs       = -1\n",
        "    )\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        **params\n",
        "    ).fit(X_train_vec, y_train_e)\n",
        "\n",
        "\n",
        "\n",
        "    # — prédictions & métriques\n",
        "    y_proba = model.predict_proba(X_test_vec)[:, 1]   # probability of class “1” (i.e. original “4”)\n",
        "    y_pred  = (y_proba > 0.5).astype(int)\n",
        "\n",
        "\n",
        "    acc = accuracy_score(y_test_e, y_pred)\n",
        "    rpt = classification_report(\n",
        "            y_test_e, y_pred,\n",
        "            target_names=[str(c) for c in encoder.classes_],\n",
        "            output_dict=True, zero_division=0\n",
        "          )\n",
        "\n",
        "    # ---- log MLflow\n",
        "    mlflow.log_params(params)\n",
        "    mlflow.log_metric(\"accuracy\", acc)\n",
        "    for lbl in ['0','4']:\n",
        "        mlflow.log_metric(f\"precision_{lbl}\", rpt[lbl]['precision'])\n",
        "        mlflow.log_metric(f\"recall_{lbl}\",    rpt[lbl]['recall'])\n",
        "        mlflow.log_metric(f\"f1_{lbl}\",        rpt[lbl]['f1-score'])\n",
        "\n",
        "    # ---- artefact & sauvegarde Drive\n",
        "    pkl_path = pathlib.Path(\n",
        "        \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/W2V_fixed.pkl\"\n",
        "    )\n",
        "    with open(pkl_path, \"wb\") as f:\n",
        "        pickle.dump(\n",
        "            {\"model\": model,\n",
        "             \"w2v\"  : w2v_model,\n",
        "             \"encoder\": encoder}, f\n",
        "        )\n",
        "    mlflow.log_artifact(str(pkl_path))\n",
        "\n",
        "    print(f\"✅  W2V_fixed — acc={acc:.4f} | f1_0={rpt['0']['f1-score']:.3f} | f1_4={rpt['4']['f1-score']:.3f}\")\n",
        "\n",
        "print(\"\\n🏁  Run terminé — retrouvez-le dans l’UI MLflow (onglet *W2V_XGBoost*).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fuser -k 5000/tcp # KILL THE SERVER"
      ],
      "metadata": {
        "id": "8cQPKiD4tCR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, output\n",
        "import shlex, os, time, subprocess, textwrap\n",
        "\n",
        "# 0) (Re)monter Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 1) Tuer toute instance éventuelle\n",
        "!fuser -k 5000/tcp || true\n",
        "\n",
        "# 2) Dossier backend\n",
        "BACKEND = \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/MLflowStore\"\n",
        "os.makedirs(BACKEND, exist_ok=True)\n",
        "\n",
        "# 3) Lancer avec log détaillé\n",
        "get_ipython().system_raw(\n",
        "    f\"mlflow server \"\n",
        "    f\"--backend-store-uri {shlex.quote(BACKEND)} \"\n",
        "    f\"--default-artifact-root {shlex.quote(BACKEND)} \"\n",
        "    f\"--host 0.0.0.0 --port 5000 --workers 1 \"\n",
        "    f\"> mlflow.log 2>&1 &\"\n",
        ")\n",
        "\n",
        "time.sleep(6)                        # laisse vraiment du temps\n",
        "\n",
        "# 4) Vérif rapide\n",
        "print(subprocess.run('lsof -i:5000', shell=True, text=True).stdout or \"Port 5000 fermé\")\n",
        "\n",
        "# 5) URL proxy\n",
        "print(\"UI MLflow :\", output.eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "21HrKhXqtQHF",
        "outputId": "cafe22fc-930c-4cb6-c154-a92597a83668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Port 5000 fermé\n",
            "UI MLflow : https://5000-m-hm-1nk77asvti4v9-a.asia-east1-0.prod.colab.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyeU806i84F0"
      },
      "source": [
        "## UNIVERSAL SENTENCE ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qMn97b_K8-yA",
        "outputId": "b82eaf6b-fcbf-4d5f-bd4a-90e4ca178b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 uninstall [options] <package> ...\n",
            "  pip3 uninstall [options] -r <requirements file> ...\n",
            "\n",
            "no such option: -U\n",
            "Collecting tensorflow==2.15.*\n",
            "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tensorflow-text==2.15.*\n",
            "  Downloading tensorflow_text-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-hub==0.16.1 in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.*)\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.*)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (4.14.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.*)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.73.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.*)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.*)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.*)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.16.1) (2.18.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.*) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.1.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub==0.16.1)\n",
            "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.3.1)\n",
            "Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, ml-dtypes, keras, tensorboard, tensorflow, tf-keras, tensorflow-text\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.1\n",
            "    Uninstalling tensorflow-2.18.1:\n",
            "      Successfully uninstalled tensorflow-2.18.1\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.18.0\n",
            "    Uninstalling tf_keras-2.18.0:\n",
            "      Successfully uninstalled tf_keras-2.18.0\n",
            "  Attempting uninstall: tensorflow-text\n",
            "    Found existing installation: tensorflow-text 2.18.1\n",
            "    Uninstalling tensorflow-text-2.18.1:\n",
            "      Successfully uninstalled tensorflow-text-2.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.1 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 tensorflow-text-2.15.0 tf-keras-2.15.1 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "keras",
                  "ml_dtypes",
                  "tensorflow",
                  "wrapt"
                ]
              },
              "id": "433b9b003d9b489499039d5c6a615c1b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!pip uninstall -U \"tensorflow==2.15.*\" \"tensorflow-text==2.15.*\" \"tensorflow-hub==0.16.1\" -y\n",
        "#!pip install \"tensorflow==2.15.*\" \"tensorflow-text==2.15.*\" \"tensorflow-hub==0.16.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou4syuSq81JA",
        "outputId": "dfe80820-b7e0-44c2-a9a3-67576b9624f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/20 16:51:43 INFO mlflow.tracking.fluent: Experiment with name 'USE_Keras' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "80000/80000 - 794s - loss: 0.5011 - accuracy: 0.7639 - val_loss: 0.4682 - val_accuracy: 0.7782 - 794s/epoch - 10ms/step\n",
            "Epoch 2/5\n",
            "80000/80000 - 776s - loss: 0.4817 - accuracy: 0.7701 - val_loss: 0.4658 - val_accuracy: 0.7793 - 776s/epoch - 10ms/step\n",
            "Epoch 3/5\n",
            "80000/80000 - 774s - loss: 0.4812 - accuracy: 0.7701 - val_loss: 0.4652 - val_accuracy: 0.7794 - 774s/epoch - 10ms/step\n",
            "Epoch 4/5\n",
            "80000/80000 - 771s - loss: 0.4813 - accuracy: 0.7700 - val_loss: 0.4649 - val_accuracy: 0.7795 - 771s/epoch - 10ms/step\n",
            "Epoch 5/5\n",
            "80000/80000 - 766s - loss: 0.4813 - accuracy: 0.7703 - val_loss: 0.4648 - val_accuracy: 0.7797 - 766s/epoch - 10ms/step\n",
            "5000/5000 [==============================] - 32s 6ms/step\n",
            "✅  USE_fixed — acc=0.7797 | f1_0=0.778 | f1_4=0.781\n",
            "🏃 View run USE_fixed at: http://127.0.0.1:5000/#/experiments/691042665796839504/runs/9937cbcd67804ae4a7ae7bc8c8e0cad2\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/691042665796839504\n",
            "\n",
            "🏁  Run terminé — retrouvez-le dans l’interface MLflow (expérience **USE_Keras**).\n"
          ]
        }
      ],
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# 0. PRÉREQUIS\n",
        "#    • serveur MLflow actif (http://127.00.1:5000 ➜ MLflowStore)\n",
        "#    • TensorFlow ≥ 2.15 + tensorflow-hub installés\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# !pip install -q --upgrade tensorflow tensorflow-hub # Already installed in previous cells\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 1. DATASET Sentiment140\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "import kagglehub, pandas as pd, mlflow, json, pathlib, pickle, os, time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "csv_file = f\"{path}/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "cols = ['target','id','date','flag','user','text']\n",
        "df = pd.read_csv(csv_file, encoding='ISO-8859-1', names=cols)\n",
        "\n",
        "texts  = df['text'].astype(str)\n",
        "labels = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc  = le.transform(y_test)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 2. UNIVERSAL SENTENCE ENCODER  & MODELE KERAS\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "\n",
        "class USELayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(USELayer, self).__init__(**kwargs)\n",
        "        self.use_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "            input_shape=[], dtype=tf.string, trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.use_layer(inputs)\n",
        "\n",
        "inputs  = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
        "x       = USELayer(name=\"USE\")(inputs) # Use the custom layer\n",
        "x       = tf.keras.layers.Dropout(0.3)(x)\n",
        "outputs = tf.keras.layers.Dense(len(le.classes_), activation=\"softmax\", name=\"classifier\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 3. MLflow — expérience USE\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "mlflow.set_experiment(\"USE_Keras\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"USE_fixed\"):\n",
        "\n",
        "    # — Paramètres notables\n",
        "    mlflow.log_param(\"dropout\", 0.3)\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"batch_size\", 16)\n",
        "    mlflow.log_param(\"encoder\", \"USE/4\")\n",
        "\n",
        "    # — Entraînement\n",
        "    history = model.fit(\n",
        "        X_train, y_train_enc,\n",
        "        validation_data=(X_test, y_test_enc),\n",
        "        epochs=5,\n",
        "        batch_size=16,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # — Évaluation finale\n",
        "    y_pred_prob = model.predict(X_test, batch_size=64)\n",
        "    y_pred      = y_pred_prob.argmax(axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    acc = accuracy_score(y_test_enc, y_pred)\n",
        "    rpt = classification_report(\n",
        "            y_test_enc, y_pred,\n",
        "            target_names=[str(c) for c in le.classes_],\n",
        "            output_dict=True, zero_division=0\n",
        "          )\n",
        "\n",
        "    mlflow.log_metric(\"accuracy\", acc)\n",
        "    for lbl in ['0','4']:\n",
        "        mlflow.log_metric(f\"precision_{lbl}\", rpt[lbl]['precision'])\n",
        "        mlflow.log_metric(f\"recall_{lbl}\",    rpt[lbl]['recall'])\n",
        "        mlflow.log_metric(f\"f1_{lbl}\",        rpt[lbl]['f1-score'])\n",
        "\n",
        "    # — Sauvegarde du modèle (SavedModel) + artefact\n",
        "    saved_path = \"/content/use_model_saved\"\n",
        "    model.save(saved_path, include_optimizer=False)\n",
        "    mlflow.log_artifact(saved_path)            # dossier entier archivé\n",
        "\n",
        "    # — Sauvegarde du label encoder\n",
        "    enc_path = pathlib.Path(\"/content/label_encoder.pkl\")\n",
        "    pickle.dump(le, open(enc_path,\"wb\"))\n",
        "    mlflow.log_artifact(str(enc_path))\n",
        "\n",
        "    print(f\"✅  USE_fixed — acc={acc:.4f} | f1_0={rpt['0']['f1-score']:.3f} | f1_4={rpt['4']['f1-score']:.3f}\")\n",
        "\n",
        "    # Copie pratique dans Drive\n",
        "    drive_pkl = \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/USE_fixed_encoder.pkl\"\n",
        "    !cp \"{enc_path}\" \"{drive_pkl}\"\n",
        "    drive_model = \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/USE_fixed_model\"\n",
        "    !cp -r \"{saved_path}\" \"{drive_model}\"\n",
        "\n",
        "\n",
        "print(\"\\n🏁  Run terminé — retrouvez-le dans l’interface MLflow (expérience **USE_Keras**).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M31ZMGuZqt9j"
      },
      "source": [
        "# lancer le serveur ML flow sans relancer les experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bXyNFEF8k7r9",
        "outputId": "f2674ca4-8e21-4ff8-9e2d-711b69c98706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Port 5000 fermé\n",
            "UI MLflow : https://5000-m-s-3inedjgsm6flj-a.us-central1-1.prod.colab.dev\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, output\n",
        "import shlex, os, time, subprocess, textwrap\n",
        "\n",
        "# 0) (Re)monter Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 1) Tuer toute instance éventuelle\n",
        "!fuser -k 5000/tcp || true\n",
        "\n",
        "# 2) Dossier backend\n",
        "BACKEND = \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/MLflowStore\"\n",
        "os.makedirs(BACKEND, exist_ok=True)\n",
        "\n",
        "# 3) Lancer avec log détaillé\n",
        "get_ipython().system_raw(\n",
        "    f\"mlflow server \"\n",
        "    f\"--backend-store-uri {shlex.quote(BACKEND)} \"\n",
        "    f\"--default-artifact-root {shlex.quote(BACKEND)} \"\n",
        "    f\"--host 0.0.0.0 --port 5000 --workers 1 \"\n",
        "    f\"> mlflow.log 2>&1 &\"\n",
        ")\n",
        "\n",
        "time.sleep(6)                        # laisse vraiment du temps\n",
        "\n",
        "# 4) Vérif rapide\n",
        "print(subprocess.run('lsof -i:5000', shell=True, text=True).stdout or \"Port 5000 fermé\")\n",
        "\n",
        "# 5) URL proxy\n",
        "print(\"UI MLflow :\", output.eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWbxyeREukJ9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsJdpGCWnhje"
      },
      "outputs": [],
      "source": [
        "!fuser -k 5000/tcp # KILL THE SERVER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM0Yy7hwM7g4"
      },
      "source": [
        "# Tracking via MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kfaOOG8cFMln",
        "outputId": "36c9f334-9331-41e1-f1ff-fa03285f7f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/24.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/24.7 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/24.7 MB\u001b[0m \u001b[31m238.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m24.5/24.7 MB\u001b[0m \u001b[31m246.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m237.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/246.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/147.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/114.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m741.4/741.4 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q mlflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EGcfI0eilT8J",
        "outputId": "98a17668-3b4e-462a-d7c2-668b769376c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 uninstall [options] <package> ...\n",
            "  pip3 uninstall [options] -r <requirements file> ...\n",
            "\n",
            "no such option: -U\n",
            "Collecting tensorflow==2.15.*\n",
            "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tensorflow-text==2.15.*\n",
            "  Downloading tensorflow_text-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-hub==0.16.1 in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.*)\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.15.*)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.*)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.*)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.*) (1.73.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.*)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.*)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.*)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.16.1) (2.18.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.*) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.1.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub==0.16.1)\n",
            "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.*) (3.3.1)\n",
            "Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, ml-dtypes, tensorboard, tensorflow, tf-keras, tensorflow-text\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.18.0\n",
            "    Uninstalling tf_keras-2.18.0:\n",
            "      Successfully uninstalled tf_keras-2.18.0\n",
            "  Attempting uninstall: tensorflow-text\n",
            "    Found existing installation: tensorflow-text 2.18.1\n",
            "    Uninstalling tensorflow-text-2.18.1:\n",
            "      Successfully uninstalled tensorflow-text-2.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, but you have tf-keras 2.15.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tf-keras~=2.17, but you have tf-keras 2.15.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 numpy-1.26.4 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 tensorflow-text-2.15.0 tf-keras-2.15.1 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "381a7fbd96ba4914a3ba3f06d203d33e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall -U \"tensorflow==2.15.*\" \"tensorflow-text==2.15.*\" \"tensorflow-hub==0.16.1\" -y\n",
        "!pip install \"tensorflow==2.15.*\" \"tensorflow-text==2.15.*\" \"tensorflow-hub==0.16.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZSVRLpC2px5L",
        "outputId": "3c679e36-9919-4b0a-c4a6-3ca009db6916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting plot-keras-history\n",
            "  Downloading plot_keras_history-1.1.39.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from plot-keras-history) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from plot-keras-history) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from plot-keras-history) (1.15.3)\n",
            "Collecting sanitize_ml_labels>=1.0.48 (from plot-keras-history)\n",
            "  Downloading sanitize_ml_labels-1.1.4.tar.gz (324 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.5/324.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting compress-json (from sanitize_ml_labels>=1.0.48->plot-keras-history)\n",
            "  Downloading compress_json-1.1.1.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->plot-keras-history) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->plot-keras-history) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->plot-keras-history) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->plot-keras-history) (1.17.0)\n",
            "Building wheels for collected packages: plot-keras-history, sanitize_ml_labels, compress-json\n",
            "  Building wheel for plot-keras-history (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for plot-keras-history: filename=plot_keras_history-1.1.39-py3-none-any.whl size=10667 sha256=4e7fe754b78b07a9142cafb38b76e6e7f65fb36d6a070d458313070776400e28\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/8d/d7/bd70289b1bd192664225cd608fd08437ecc725c3f8918383d9\n",
            "  Building wheel for sanitize_ml_labels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sanitize_ml_labels: filename=sanitize_ml_labels-1.1.4-py3-none-any.whl size=324285 sha256=caa8981300629f88aed8559e1dc81c3f608cdcff77dd3ba5bdad7613cfe2f2d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/32/52/40db53b628215fe3c4fe7d0b0fe1decfd67ccccc91118df507\n",
            "  Building wheel for compress-json (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compress-json: filename=compress_json-1.1.1-py3-none-any.whl size=6598 sha256=88664ebb1932093e1894047bb2a4a6142cdc1a60221ebaeed34f7b160f5d4c42\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/7a/5f/bd24248a3baef974b40320ec83d3b1ae1b620d0e48d899625e\n",
            "Successfully built plot-keras-history sanitize_ml_labels compress-json\n",
            "Installing collected packages: compress-json, sanitize_ml_labels, plot-keras-history\n",
            "Successfully installed compress-json-1.1.1 plot-keras-history-1.1.39 sanitize_ml_labels-1.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install plot-keras-history"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9bC-3gvlVJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa64PhvxkTdb"
      },
      "source": [
        "# DistilBERT : version 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 0. Install & MLflow                         │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "from google.colab import drive, output\n",
        "import os, shlex, time, pickle, pathlib\n",
        "import pandas as pd, numpy as np, mlflow, kagglehub\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ── petite fonction de courbe ───────────────────────────\n",
        "def save_history(history, path, title):\n",
        "    fig, ax1 = plt.subplots(figsize=(6,4))\n",
        "    ax1.set_title(title)\n",
        "    ax1.plot(history.history[\"loss\"],        label=\"loss\")\n",
        "    ax1.plot(history.history[\"val_loss\"],    label=\"val_loss\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(history.history[\"accuracy\"],     label=\"acc\",      c=\"g\", ls=\"--\")\n",
        "    ax2.plot(history.history[\"val_accuracy\"], label=\"val_acc\",  c=\"r\", ls=\"--\")\n",
        "    ax1.set_xlabel(\"epoch\"); ax1.set_ylabel(\"loss\"); ax2.set_ylabel(\"acc\")\n",
        "    lines = ax1.get_lines()+ax2.get_lines()\n",
        "    fig.legend(lines, [l.get_label() for l in lines], loc=\"lower right\")\n",
        "    fig.tight_layout(); fig.savefig(path); plt.close(fig)\n",
        "\n",
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 1. Drive + MLflow                           │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "PORT, BACKEND = 5000, \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/MLflowStore\"\n",
        "!fuser -k {PORT}/tcp || true\n",
        "os.makedirs(BACKEND, exist_ok=True)\n",
        "get_ipython().system_raw(\n",
        "    f\"mlflow server --backend-store-uri {shlex.quote(BACKEND)} \"\n",
        "    f\"--default-artifact-root {shlex.quote(BACKEND)} \"\n",
        "    f\"--host 0.0.0.0 --port {PORT} --workers 1 &\")\n",
        "time.sleep(3)\n",
        "\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "mlflow.set_experiment(\"BERT_curriculum_distil_HF\")\n",
        "print(\"MLflow UI:\", output.eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "\n",
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 2. Dataset                                  │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "df = pd.read_csv(\n",
        "    kagglehub.dataset_download(\"kazanova/sentiment140\") +\n",
        "    \"/training.1600000.processed.noemoticon.csv\",\n",
        "    names=[\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"],\n",
        "    encoding=\"ISO-8859-1\"\n",
        ")\n",
        "texts, labels = df[\"text\"].astype(str).values, df[\"target\"].values\n",
        "le = LabelEncoder(); labels_enc = le.fit_transform(labels)\n",
        "\n",
        "VAL_SIZE = 10_000\n",
        "val_split = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=123)\n",
        "train_pool_idx, val_idx = next(val_split.split(texts, labels_enc))\n",
        "X_val, y_val = texts[val_idx], labels_enc[val_idx]\n",
        "train_pool   = train_pool_idx                     # encore disponibles\n",
        "\n",
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 3. DistilBERT (TF + HF)                     │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "MODEL = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "bert      = TFAutoModel.from_pretrained(MODEL)\n",
        "\n",
        "def encode(batch):\n",
        "    toks = tokenizer(list(batch), truncation=True, padding=\"max_length\",\n",
        "                     max_length=128, return_tensors=\"tf\")\n",
        "    return toks[\"input_ids\"], toks[\"attention_mask\"]\n",
        "\n",
        "def make_ds(x, y, shuffle=False):\n",
        "    ids, mask = encode(x)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((ids, mask), y.astype(np.float32)))\n",
        "    if shuffle: ds = ds.shuffle(len(x), reshuffle_each_iteration=True)\n",
        "    return ds.batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = make_ds(X_val, y_val)\n",
        "\n",
        "def build_model():\n",
        "    ids_in  = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"ids\")\n",
        "    mask_in = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"mask\")\n",
        "    x       = bert(ids_in, attention_mask=mask_in).last_hidden_state[:,0,:]\n",
        "    x       = tf.keras.layers.Dropout(0.1)(x)\n",
        "    out     = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model   = tf.keras.Model([ids_in, mask_in], out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(3e-5),\n",
        "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = build_model()            # UNE SEULE FOIS\n",
        "\n",
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 4. Curriculum (même modèle, pas de reload)   │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "STAGES = [10_000, 25_000, 50_000, 100_000, 125_000, 150_000]\n",
        "EPOCHS_PER_STAGE = 2\n",
        "ckpt_root = pathlib.Path(\"/content/drive/MyDrive/Colab_Notebooks/PROJET7/bert_curriculum_HF\")\n",
        "ckpt_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "total_epochs, prev_run_id, prev_size = 0, None, 0\n",
        "\n",
        "for size in STAGES:\n",
        "    add_needed = size - prev_size\n",
        "    add_split  = StratifiedShuffleSplit(n_splits=1, train_size=add_needed,\n",
        "                                        random_state=42+size)\n",
        "    add_rel, _ = next(add_split.split(texts[train_pool], labels_enc[train_pool]))\n",
        "    add_idx    = train_pool[add_rel]\n",
        "    train_pool = np.setdiff1d(train_pool, add_idx)\n",
        "\n",
        "    X_add, y_add = texts[add_idx], labels_enc[add_idx]\n",
        "    train_ds = make_ds(X_add, y_add, shuffle=True)\n",
        "\n",
        "    with mlflow.start_run(run_name=f\"DistilHF_{size//1000}k\",\n",
        "                          nested=True,\n",
        "                          tags={\"parent\": prev_run_id} if prev_run_id else None) as run:\n",
        "\n",
        "        mlflow.log_params({\"cumulative_train\": size,\n",
        "                           \"added_this_stage\": add_needed,\n",
        "                           \"epochs_this_stage\": EPOCHS_PER_STAGE})\n",
        "\n",
        "        hist = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=total_epochs + EPOCHS_PER_STAGE,\n",
        "            initial_epoch=total_epochs,\n",
        "            verbose=2,\n",
        "            callbacks=[tf.keras.callbacks.LambdaCallback(\n",
        "                on_epoch_end=lambda e,l: mlflow.log_metric(\n",
        "                    \"val_accuracy\", float(l[\"val_accuracy\"]), step=e))]\n",
        "        )\n",
        "\n",
        "        total_epochs += EPOCHS_PER_STAGE\n",
        "        prev_size     = size\n",
        "\n",
        "        png = f\"/tmp/hist_{size}.png\"\n",
        "        save_history(hist, png, title=f\"DistilBERT {size//1000}k\")\n",
        "        mlflow.log_artifact(png)\n",
        "\n",
        "        ckpt_path = ckpt_root / f\"distilbert_HF_{size}k\"\n",
        "        model.save(ckpt_path, include_optimizer=True)\n",
        "        mlflow.log_artifact(str(ckpt_path))\n",
        "\n",
        "        if size == STAGES[0]:\n",
        "            enc_pkl = ckpt_root / \"label_encoder.pkl\"\n",
        "            pickle.dump(le, open(enc_pkl,\"wb\"))\n",
        "            mlflow.log_artifact(str(enc_pkl))\n",
        "\n",
        "        prev_run_id = run.info.run_id\n",
        "        print(f\"✅  Stage {size//1000}k — val_acc={hist.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "print(\"\\n🏁  Curriculum terminé — tout est dans MLflow.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UimS-iuW2bK_",
        "outputId": "01d23871-e9cf-4409-d9f7-585e7172ecc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "MLflow UI: https://5000-gpu-l4-s-x38m43v3qpyy-c.us-west4-0.prod.colab.dev\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 - 1007s - loss: 0.4797 - accuracy: 0.7724 - val_loss: 0.3957 - val_accuracy: 0.8187 - 1007s/epoch - 2s/step\n",
            "Epoch 2/2\n",
            "625/625 - 991s - loss: 0.3077 - accuracy: 0.8694 - val_loss: 0.4437 - val_accuracy: 0.8022 - 991s/epoch - 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d8510>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4960d5310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959c2210>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d495991410>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d49591f150>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d9650>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Stage 10k — val_acc=0.8022\n",
            "🏃 View run DistilHF_10k at: http://127.0.0.1:5000/#/experiments/281729971456783880/runs/9f2d7de07cc743b8aa81de28e02593ca\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/281729971456783880\n",
            "Epoch 3/4\n",
            "938/938 - 1374s - loss: 0.4193 - accuracy: 0.8113 - val_loss: 0.3905 - val_accuracy: 0.8277 - 1374s/epoch - 1s/step\n",
            "Epoch 4/4\n",
            "938/938 - 1369s - loss: 0.2701 - accuracy: 0.8887 - val_loss: 0.4471 - val_accuracy: 0.8154 - 1369s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d8510>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4960d5310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959c2210>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d495991410>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d49591f150>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d9650>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Stage 25k — val_acc=0.8154\n",
            "🏃 View run DistilHF_25k at: http://127.0.0.1:5000/#/experiments/281729971456783880/runs/c18c65e885864b52b881ca028fc87cc1\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/281729971456783880\n",
            "Epoch 5/6\n",
            "1563/1563 - 2132s - loss: 0.3999 - accuracy: 0.8204 - val_loss: 0.3741 - val_accuracy: 0.8310 - 2132s/epoch - 1s/step\n",
            "Epoch 6/6\n",
            "1563/1563 - 2129s - loss: 0.2725 - accuracy: 0.8862 - val_loss: 0.4196 - val_accuracy: 0.8242 - 2129s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d8510>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4960d5310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959c2210>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d495991410>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d49591f150>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d9650>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Stage 50k — val_acc=0.8242\n",
            "🏃 View run DistilHF_50k at: http://127.0.0.1:5000/#/experiments/281729971456783880/runs/d4193a2d16124d1585c329970d1d6b5c\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/281729971456783880\n",
            "Epoch 7/8\n",
            "3125/3125 - 4031s - loss: 0.3810 - accuracy: 0.8310 - val_loss: 0.3879 - val_accuracy: 0.8282 - 4031s/epoch - 1s/step\n",
            "Epoch 8/8\n",
            "3125/3125 - 4036s - loss: 0.2690 - accuracy: 0.8900 - val_loss: 0.3900 - val_accuracy: 0.8349 - 4036s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d8510>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4960d5310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959c2210>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d495991410>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d49591f150>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d9650>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Stage 100k — val_acc=0.8349\n",
            "🏃 View run DistilHF_100k at: http://127.0.0.1:5000/#/experiments/281729971456783880/runs/996c0636489c44b7af62c8ccb25a4242\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/281729971456783880\n",
            "Epoch 9/10\n",
            "1563/1563 - 2134s - loss: 0.3821 - accuracy: 0.8280 - val_loss: 0.3551 - val_accuracy: 0.8419 - 2134s/epoch - 1s/step\n",
            "Epoch 10/10\n",
            "1563/1563 - 2136s - loss: 0.2474 - accuracy: 0.8989 - val_loss: 0.3952 - val_accuracy: 0.8330 - 2136s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d8510>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4960d5310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959c2210>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d495991410>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d49591f150>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d9650>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Stage 125k — val_acc=0.8330\n",
            "🏃 View run DistilHF_125k at: http://127.0.0.1:5000/#/experiments/281729971456783880/runs/05fc8948e6fe4c659391fd9c9d8f5d4d\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/281729971456783880\n",
            "Epoch 11/12\n",
            "1563/1563 - 2135s - loss: 0.3809 - accuracy: 0.8297 - val_loss: 0.3627 - val_accuracy: 0.8399 - 2135s/epoch - 1s/step\n",
            "Epoch 12/12\n",
            "1563/1563 - 2133s - loss: 0.2373 - accuracy: 0.9051 - val_loss: 0.4049 - val_accuracy: 0.8353 - 2133s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d8510>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4960d5310>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959c2210>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d495991410>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d49591f150>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x78d4959d9650>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Stage 150k — val_acc=0.8353\n",
            "🏃 View run DistilHF_150k at: http://127.0.0.1:5000/#/experiments/281729971456783880/runs/f0cac6b2e42549b0b762e4681c239d5e\n",
            "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/281729971456783880\n",
            "\n",
            "🏁  Curriculum terminé — tout est dans MLflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT : version 2\n",
        "\n",
        "version utilisée en production"
      ],
      "metadata": {
        "id": "PJsslm-W1cGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"numpy<2.0,>=1.26\" \\\n",
        "               \"tensorflow==2.16.*\" \\\n",
        "               \"keras==3.*\" \\\n",
        "               \"transformers>=4.41\" \\\n",
        "               \"mlflow>=3.1\" \\\n",
        "               kagglehub \\\n",
        "               scikit-learn \\\n",
        "               matplotlib \\\n",
        "               pandas \\\n",
        "               fsspec[s3] \\\n",
        "               plot-keras-history"
      ],
      "metadata": {
        "id": "9lyRGygq3sBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 0‑b. Imports & utilitaires                   │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "from google.colab import drive, output\n",
        "import os, shlex, time, pickle, pathlib\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import mlflow, kagglehub\n",
        "from plot_keras_history import plot_history\n",
        "\n",
        "\n",
        "# ── fonction perso (encore utile si besoin) ───\n",
        "def save_history(history, path, title):\n",
        "    fig, ax1 = plt.subplots(figsize=(6,4))\n",
        "    ax1.set_title(title)\n",
        "    ax1.plot(history.history[\"loss\"],        label=\"loss\")\n",
        "    ax1.plot(history.history[\"val_loss\"],    label=\"val_loss\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(history.history[\"accuracy\"],     label=\"acc\",      c=\"g\", ls=\"--\")\n",
        "    ax2.plot(history.history[\"val_accuracy\"], label=\"val_acc\",  c=\"r\", ls=\"--\")\n",
        "    ax1.set_xlabel(\"epoch\"); ax1.set_ylabel(\"loss\"); ax2.set_ylabel(\"acc\")\n",
        "    lines = ax1.get_lines()+ax2.get_lines()\n",
        "    fig.legend(lines, [l.get_label() for l in lines], loc=\"lower right\")\n",
        "    fig.tight_layout(); fig.savefig(path); plt.close(fig)\n"
      ],
      "metadata": {
        "id": "RtFj3sRz2EKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 1. Google Drive + MLflow                     │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "#drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()           # (au cas où un montage fantôme traîne)\n",
        "\n",
        "!rm -rf /content/drive              # nettoyer le point de montage\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "PORT, BACKEND = 5000, \"/content/drive/MyDrive/Colab_Notebooks/PROJET7/MLflowStore\"\n",
        "!fuser -k {PORT}/tcp || true\n",
        "os.makedirs(BACKEND, exist_ok=True)\n",
        "get_ipython().system_raw(\n",
        "    f\"mlflow server --backend-store-uri {shlex.quote(BACKEND)} \"\n",
        "    f\"--default-artifact-root {shlex.quote(BACKEND)} \"\n",
        "    f\"--host 0.0.0.0 --port {PORT} --workers 1 &\")\n",
        "time.sleep(3)\n",
        "\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "mlflow.set_experiment(\"BERT_curriculum_distil_HF_Last_v\")\n",
        "print(\"MLflow UI:\", output.eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n"
      ],
      "metadata": {
        "id": "TEFhu2zN2HJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 2. Dataset Sentiment140                      │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "df = pd.read_csv(\n",
        "    kagglehub.dataset_download(\"kazanova/sentiment140\") +\n",
        "    \"/training.1600000.processed.noemoticon.csv\",\n",
        "    names=[\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"],\n",
        "    encoding=\"ISO-8859-1\"\n",
        ")\n",
        "texts, labels  = df[\"text\"].astype(str).values, df[\"target\"].values\n",
        "le             = LabelEncoder()\n",
        "labels_enc     = le.fit_transform(labels)\n",
        "\n",
        "VAL_SIZE = 10_000\n",
        "val_split      = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=123)\n",
        "train_pool_idx, val_idx = next(val_split.split(texts, labels_enc))\n",
        "X_val, y_val   = texts[val_idx], labels_enc[val_idx]\n",
        "train_pool     = train_pool_idx.copy()                # échantillons restants\n"
      ],
      "metadata": {
        "id": "JLDgqMUF2Juc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 3. Modèle DistilBERT (TF + HF)               │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "MODEL     = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "bert      = TFAutoModel.from_pretrained(MODEL)\n",
        "\n",
        "def encode(batch):\n",
        "    toks = tokenizer(list(batch),\n",
        "                     truncation=True, padding=\"max_length\",\n",
        "                     max_length=128, return_tensors=\"tf\")\n",
        "    return toks[\"input_ids\"], toks[\"attention_mask\"]\n",
        "\n",
        "def make_ds(x, y, shuffle=False):\n",
        "    ids, mask = encode(x)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((ids, mask), y.astype(np.float32)))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(len(x), reshuffle_each_iteration=True)\n",
        "    return ds.batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = make_ds(X_val, y_val)\n",
        "\n",
        "def build_model():\n",
        "    ids_in  = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"ids\")\n",
        "    mask_in = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"mask\")\n",
        "    x       = bert(ids_in, attention_mask=mask_in).last_hidden_state[:, 0, :]\n",
        "    x       = tf.keras.layers.Dropout(0.1)(x)\n",
        "    out     = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model   = tf.keras.Model([ids_in, mask_in], out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(3e-5),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n"
      ],
      "metadata": {
        "id": "kpghy_E42MIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╭──────────────────────────────────────────────╮\n",
        "# │ 4. Curriculum learning + plot‑keras‑history  │\n",
        "# ╰──────────────────────────────────────────────╯\n",
        "STAGES            = [25_000, 50_000, 100_000, 125_000, 150_000,175_000,200_000]          # exemples cumulés (à ajuster)\n",
        "EPOCHS_PER_STAGE  = 2\n",
        "ckpt_root         = pathlib.Path(\"/content/drive/MyDrive/Colab_Notebooks/PROJET7/bert_curriculum_HF_last_version\")\n",
        "ckpt_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "total_epochs, prev_run_id, prev_size = 0, None, 0\n",
        "\n",
        "for size in STAGES:\n",
        "    # -- sélection des nouveaux exemples ----------------------------\n",
        "    add_needed = size - prev_size\n",
        "    add_split  = StratifiedShuffleSplit(\n",
        "        n_splits=1, train_size=add_needed, random_state=42 + size\n",
        "    )\n",
        "    add_rel, _ = next(add_split.split(texts[train_pool], labels_enc[train_pool]))\n",
        "    add_idx    = train_pool[add_rel]\n",
        "    train_pool = np.setdiff1d(train_pool, add_idx)\n",
        "\n",
        "    X_add, y_add = texts[add_idx], labels_enc[add_idx]\n",
        "    train_ds     = make_ds(X_add, y_add, shuffle=True)\n",
        "\n",
        "    # -- run MLflow --------------------------------------------------\n",
        "    with mlflow.start_run(\n",
        "        run_name=f\"DistilHF_{size//1000}k\",\n",
        "        nested=True,\n",
        "        tags={\"parent\": prev_run_id} if prev_run_id else None,\n",
        "    ) as run:\n",
        "\n",
        "        mlflow.log_params({\n",
        "            \"cumulative_train\":  size,\n",
        "            \"added_this_stage\":  add_needed,\n",
        "            \"epochs_this_stage\": EPOCHS_PER_STAGE,\n",
        "        })\n",
        "\n",
        "        hist = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=total_epochs + EPOCHS_PER_STAGE,\n",
        "            initial_epoch=total_epochs,\n",
        "            verbose=2,\n",
        "            callbacks=[tf.keras.callbacks.LambdaCallback(\n",
        "                on_epoch_end=lambda e, l: mlflow.log_metric(\n",
        "                    \"val_accuracy\", float(l[\"val_accuracy\"]), step=e)\n",
        "            )],\n",
        "        )\n",
        "\n",
        "        total_epochs += EPOCHS_PER_STAGE\n",
        "        prev_size     = size\n",
        "\n",
        "        # -- tracé avec plot‑keras‑history ---------------------------\n",
        "        png = f\"/tmp/hist_{size}.png\"\n",
        "        png = f\"/tmp/hist_{size}.png\"\n",
        "        png = f\"/tmp/hist_{size}.png\"\n",
        "        fig, _ = plot_history(          # ← déballer le tuple\n",
        "            hist.history,\n",
        "            path  = png,\n",
        "            title = f\"DistilBERT {size//1000}k\"\n",
        "        )\n",
        "        mlflow.log_artifact(png)\n",
        "        plt.close(fig)                  # maintenant c’est bien une Figure\n",
        "\n",
        "\n",
        "        # -- sauvegarde du modèle -----------------------------------\n",
        "        ckpt_path = ckpt_root / f\"distilbert_HF_{size}k\"\n",
        "        model.save(ckpt_path, save_format=\"tf\")\n",
        "        mlflow.log_artifact(str(ckpt_path))\n",
        "\n",
        "        # -- pickling du label encoder au 1er stage ------------------\n",
        "        if size == STAGES[0]:\n",
        "            enc_pkl = ckpt_root / \"label_encoder.pkl\"\n",
        "            pickle.dump(le, open(enc_pkl, \"wb\"))\n",
        "            mlflow.log_artifact(str(enc_pkl))\n",
        "\n",
        "        prev_run_id = run.info.run_id\n",
        "        print(f\"✅  Stage {size//1000}k — val_acc={hist.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "print(\"\\n🏁  Curriculum terminé — tout est dans MLflow.\")\n"
      ],
      "metadata": {
        "id": "_qVdgfhM2OSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/2\n",
        "1563/1563 - 2158s - loss: 0.4436 - accuracy: 0.7934 - val_loss: 0.4037 - val_accuracy: 0.8142 - 2158s/epoch - 1s/step\n",
        "Epoch 2/2\n",
        "1563/1563 - 2128s - loss: 0.2994 - accuracy: 0.8738 - val_loss: 0.3978 - val_accuracy: 0.8276 - 2128s/epoch - 1s/step\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f086f1550>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f94105e90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f67f28590>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d703468dd90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f08406110>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f0849c250>, because it is not built.\n",
        "✅  Stage 25k — val_acc=0.8276\n",
        "🏃 View run DistilHF_25k at: http://127.0.0.1:5000/#/experiments/591348536328332306/runs/9c6dcfe3209b4d9f8c5b6ec149322996\n",
        "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/591348536328332306\n",
        "Epoch 3/4\n",
        "1563/1563 - 2132s - loss: 0.3925 - accuracy: 0.8251 - val_loss: 0.3752 - val_accuracy: 0.8310 - 2132s/epoch - 1s/step\n",
        "Epoch 4/4\n",
        "1563/1563 - 2132s - loss: 0.2594 - accuracy: 0.8936 - val_loss: 0.3969 - val_accuracy: 0.8337 - 2132s/epoch - 1s/step\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f086f1550>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f94105e90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f67f28590>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d703468dd90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f08406110>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f0849c250>, because it is not built.\n",
        "✅  Stage 50k — val_acc=0.8337\n",
        "🏃 View run DistilHF_50k at: http://127.0.0.1:5000/#/experiments/591348536328332306/runs/6e17d4dd719d4d70ad97aa4922c77d0e\n",
        "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/591348536328332306\n",
        "Epoch 5/6\n",
        "3125/3125 - 4051s - loss: 0.3845 - accuracy: 0.8271 - val_loss: 0.3563 - val_accuracy: 0.8402 - 4051s/epoch - 1s/step\n",
        "Epoch 6/6\n",
        "3125/3125 - 4011s - loss: 0.2727 - accuracy: 0.8860 - val_loss: 0.3742 - val_accuracy: 0.8404 - 4011s/epoch - 1s/step\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f086f1550>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f94105e90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f67f28590>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d703468dd90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f08406110>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f0849c250>, because it is not built.\n",
        "✅  Stage 100k — val_acc=0.8404\n",
        "🏃 View run DistilHF_100k at: http://127.0.0.1:5000/#/experiments/591348536328332306/runs/5b3a8078dd324e858de98981e2c5dbe1\n",
        "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/591348536328332306\n",
        "Epoch 7/8\n",
        "1563/1563 - 2114s - loss: 0.3708 - accuracy: 0.8317 - val_loss: 0.3547 - val_accuracy: 0.8398 - 2114s/epoch - 1s/step\n",
        "Epoch 8/8\n",
        "1563/1563 - 2109s - loss: 0.2322 - accuracy: 0.9042 - val_loss: 0.4206 - val_accuracy: 0.8389 - 2109s/epoch - 1s/step\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f086f1550>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f94105e90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f67f28590>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d703468dd90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f08406110>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f0849c250>, because it is not built.\n",
        "✅  Stage 125k — val_acc=0.8389\n",
        "🏃 View run DistilHF_125k at: http://127.0.0.1:5000/#/experiments/591348536328332306/runs/6341c06483c240aaa2c3c60c71053b31\n",
        "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/591348536328332306\n",
        "Epoch 9/10\n",
        "1563/1563 - 2110s - loss: 0.3804 - accuracy: 0.8303 - val_loss: 0.3589 - val_accuracy: 0.8426 - 2110s/epoch - 1s/step\n",
        "Epoch 10/10\n",
        "1563/1563 - 2109s - loss: 0.2433 - accuracy: 0.9010 - val_loss: 0.4132 - val_accuracy: 0.8372 - 2109s/epoch - 1s/step\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f086f1550>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f94105e90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f67f28590>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d703468dd90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f08406110>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f0849c250>, because it is not built.\n",
        "✅  Stage 150k — val_acc=0.8372\n",
        "🏃 View run DistilHF_150k at: http://127.0.0.1:5000/#/experiments/591348536328332306/runs/a47798ac6bb443d78aac0b8f33f221ed\n",
        "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/591348536328332306\n",
        "Epoch 11/12\n",
        "1563/1563 - 2111s - loss: 0.3708 - accuracy: 0.8332 - val_loss: 0.3504 - val_accuracy: 0.8438 - 2111s/epoch - 1s/step\n",
        "Epoch 12/12\n",
        "1563/1563 - 2110s - loss: 0.2286 - accuracy: 0.9070 - val_loss: 0.4246 - val_accuracy: 0.8356 - 2110s/epoch - 1s/step\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f086f1550>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f94105e90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f67f28590>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d703468dd90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f08406110>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f0849c250>, because it is not built.\n",
        "✅  Stage 175k — val_acc=0.8356\n",
        "🏃 View run DistilHF_175k at: http://127.0.0.1:5000/#/experiments/591348536328332306/runs/e4c0a174350f4406ad72bf32569d530b\n",
        "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/591348536328332306\n",
        "Epoch 13/14\n",
        "1563/1563 - 2110s - loss: 0.3758 - accuracy: 0.8330 - val_loss: 0.3440 - val_accuracy: 0.8494 - 2110s/epoch - 1s/step\n",
        "Epoch 14/14\n",
        "1563/1563 - 2109s - loss: 0.2352 - accuracy: 0.9043 - val_loss: 0.3930 - val_accuracy: 0.8371 - 2109s/epoch - 1s/step\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f086f1550>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f94105e90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f67f28590>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d703468dd90>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f08406110>, because it is not built.\n",
        "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x7d6f0849c250>, because it is not built.\n",
        "✅  Stage 200k — val_acc=0.8371\n",
        "🏃 View run DistilHF_200k at: http://127.0.0.1:5000/#/experiments/591348536328332306/runs/26a8db95e5b34c2789dc5b32d03a6b81\n",
        "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/591348536328332306\n",
        "\n",
        "🏁  Curriculum terminé — tout est dans MLflow."
      ],
      "metadata": {
        "id": "vQ-OCxLc2QiX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mReRAakn2jdf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}